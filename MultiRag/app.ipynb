{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6743f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e509f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIKIPEDIA BASED TOOLS:-1\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=5,doc_content_char_limit=500)\n",
    "wikipedia=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d94fe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARXIV BASED TOOLS:-2\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=5,doc_content_char_limit=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60087696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WEBBASED LOADER TOOLS:-3\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.tools.retriever import create_retriever_tool\n",
    "\n",
    "loader=WebBaseLoader(\"https://modelcontextprotocol.io/docs/getting-started/intro\")\n",
    "document=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(document)\n",
    "db=Chroma(collection_name=\"MultiRag\",embedding_function=HuggingFaceEmbeddings())\n",
    "doc_id=db.add_documents(text_splitter)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "web_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"model context protocol\",\n",
    "    \"Search for information about Model Context Protocol. For any questions about Model Context Protocol, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd1a0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools=[wikipedia,arxiv,web_tool]\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "llm=ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt(request: ModelRequest)-> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    docs_content = \"\\n\\n\".join(f\"{doc.page_content}\\nMetadata: {doc.metadata}\" for doc in document)\n",
    "    system_message = (\n",
    "        \"\"\"You are a helpful AI assistant. Use the following context to answer the question.\n",
    "        Think step by step before providing a detailed answer.\"\"\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "    return system_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82a3b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "agent=create_agent(llm,tools=Tools,middleware=[prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3caa52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='What meaning of MCP', additional_kwargs={}, response_metadata={}, id='1c7f7a95-c055-4fa0-a9de-32e4f40b745b'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:49:41.74554Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2370645083, 'load_duration': 56158375, 'prompt_eval_count': 902, 'prompt_eval_duration': 1757769833, 'eval_count': 20, 'eval_duration': 553686917, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--a62ddcdb-72b9-404c-867c-6f10364411c1-0', tool_calls=[{'name': 'model context protocol', 'args': {'query': 'meaning of MCP'}, 'id': '9d71a793-7720-4ce6-b668-a299c52ab917', 'type': 'tool_call'}], usage_metadata={'input_tokens': 902, 'output_tokens': 20, 'total_tokens': 922}), ToolMessage(content='Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\\n\\nThink of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.\\n\\nThink of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect electronic devices, MCP provides a standardized way to connect AI applications to external systems.', name='model context protocol', id='ca6dfe76-a3a7-4fd2-b9c9-6b23f371594f', tool_call_id='9d71a793-7720-4ce6-b668-a299c52ab917'), AIMessage(content='The meaning of MCP is the Model Context Protocol, an open-source standard for connecting AI applications to external systems. It enables AI applications to access key information and perform tasks by connecting to data sources, tools, and workflows in a standardized way.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:49:43.867222Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1917517875, 'load_duration': 59446667, 'prompt_eval_count': 750, 'prompt_eval_duration': 481706167, 'eval_count': 49, 'eval_duration': 1373928583, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--153b8443-3fae-44b5-b21b-5a6adc555396-0', usage_metadata={'input_tokens': 750, 'output_tokens': 49, 'total_tokens': 799})]}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What meaning of MCP\"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f714b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='Explain about Donald Trump family background', additional_kwargs={}, response_metadata={}, id='c079b01a-a4d8-42c1-991e-4b59fabb5cd1'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:49:47.610638Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1652622959, 'load_duration': 48717292, 'prompt_eval_count': 904, 'prompt_eval_duration': 1051051334, 'eval_count': 20, 'eval_duration': 551104875, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--52ce980b-b27e-4ae0-ba21-947c97dcc3b2-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Donald Trump family background'}, 'id': 'b6268e05-9700-4f8e-b882-0163588e0015', 'type': 'tool_call'}], usage_metadata={'input_tokens': 904, 'output_tokens': 20, 'total_tokens': 924}), ToolMessage(content=\"Page: Donald Trump\\nSummary: Donald John Trump (born June 14, 1946) is an American politician, media personality, and businessman who is the 47th president of the United States. A member of the Republican Party, he served as the 45th president from 2017 to 2021.\\nBorn into a wealthy family in New York City, Trump graduated from the University of Pennsylvania in 1968 with a bachelor's degree in economics. He became the president of his family's real estate business in 1971—inheriting the position from his father, Fred Trump—and renamed it The Trump Organization. He began acquiring and building skyscrapers, hotels, casinos, and golf courses. He also launched side ventures, many licensing the Trump name, and filed for six business bankruptcies in the 1990s and 2000s. From 2004 to 2015, he hosted the reality television show The Apprentice, bolstering his fame as a billionaire. Presenting himself as a political outsider, Trump won the 2016 presidential election against Democratic Party nominee Hillary Clinton.\\nDuring his first presidency, Trump imposed a travel ban on seven Muslim-majority countries, expanded the Mexico–United States border wall, and enforced a family separation policy on the border. He rolled back environmental and business regulations, signed the Tax Cuts and Jobs Act, and appointed three Supreme Court justices. In foreign policy, Trump withdrew the U.S. from agreements on climate, trade, and Iran's nuclear program, and initiated a trade war with China. In response to the COVID-19 pandemic from 2020, he downplayed its severity, contradicted health officials, and signed the CARES Act. After losing the 2020 presidential election to Joe Biden, Trump attempted to overturn the result, culminating in the January 6 Capitol attack in 2021. He was impeached in 2019 for abuse of power and obstruction of Congress, and in 2021 for incitement of insurrection; the Senate acquitted him both times.\\nIn 2023, Trump was found liable in civil cases for sexual abuse and defamation and for business fraud. He was found guilty of falsifying business records in 2024, making him the first U.S. president convicted of a felony. After winning the 2024 presidential election against Kamala Harris, he was sentenced to a penalty-free discharge, and two felony indictments against him for retention of classified documents and obstruction of the 2020 election were dismissed without prejudice. A racketeering case related to the 2020 election in Georgia is pending.\\nTrump began his second presidency by initiating mass layoffs of federal workers. He imposed tariffs on nearly all countries at the highest level since the Great Depression and signed the One Big Beautiful Bill Act. His administration's actions—including intimidation of political opponents and civil society, reversal of pro-diversity policies, targeting of transgender people, deportations of immigrants, extensive use of executive orders, and his broad interpretation of the unitary executive theory—have drawn over 300 lawsuits challenging the legality and constitutionality of the actions.\\nSince 2015, Trump's leadership style and political agenda—often referred to as Trumpism—have reshaped the Republican Party's identity. Many of his comments and actions have been characterized as racist or misogynistic,  and he has made false or misleading statements and promoted conspiracy theories to a degree unprecedented in American politics. Trump's actions, especially in his second term, have been described as authoritarian and contributing to democratic backsliding. After his first term, scholars and historians ranked him as one of the worst presidents in American history.\\n\\n\\n\\nPage: Fred Trump\\nSummary: Frederick Christ Trump Sr. (October 11, 1905 – June 25, 1999) was an American real-estate developer and businessman. He was the father of Donald Trump, the 45th and 47th president of the United States, along with four other children.\\nBorn in the Bronx in New York City to German immigrant parents, Trump beg\", name='wikipedia', id='19168138-6f39-4e3b-8b88-6383a6c6fcb1', tool_call_id='b6268e05-9700-4f8e-b882-0163588e0015'), AIMessage(content=\"Based on the provided Wikipedia information, here is a detailed explanation of Donald Trump's family background:\\n\\nDonald John Trump was born on June 14, 1946, into a wealthy family in New York City. His father, Fred Trump (1905-1999), was an American real estate developer and businessman who inherited his wealth from his own father, Frederick Christ Trump Sr.\\n\\nFred Trump Sr. was a successful real estate developer who built many apartment buildings in the Bronx and Brooklyn during the 1920s to 1950s. He also developed several shopping centers and office buildings. Under Fred Trump's leadership, the family business expanded significantly, becoming one of the largest private developers in New York City.\\n\\nDonald Trump's early life was marked by his father's influence on him. In 1971, Donald became the president of his family's real estate business, The Trump Organization, after his father passed away. He renamed the company and expanded its operations to become a global brand.\\n\\nFred Trump's children included Donald Trump (the 45th President of the United States) and four other siblings: Mary Anne MacLeod, Elizabeth Trump Grau, Frederick Christ Trump Jr., and Robert S. Trump.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:50:01.920073Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10358557500, 'load_duration': 58297916, 'prompt_eval_count': 1460, 'prompt_eval_duration': 2698903042, 'eval_count': 243, 'eval_duration': 7597903750, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--f175501c-e761-4de7-9f9c-f5d4aa0dbeb2-0', usage_metadata={'input_tokens': 1460, 'output_tokens': 243, 'total_tokens': 1703})]}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Explain about Donald Trump family background\"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4802069e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools Used: arxiv\n",
      "\n",
      "Agent Response:\n",
      " It appears that there are no research papers specifically about the \"Transformer model\" as requested. However, I can provide a general overview of the Transformer model and its applications.\n",
      "\n",
      "The Transformer is a neural network architecture introduced in 2017 by Vaswani et al. (1) that revolutionized natural language processing (NLP). It replaced traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention mechanisms, allowing the model to attend to all positions in a sequence simultaneously.\n",
      "\n",
      "Since its introduction, the Transformer has been widely adopted in various NLP tasks, such as machine translation, text classification, question answering, and language modeling. It has also been applied to other domains, including computer vision and speech recognition.\n",
      "\n",
      "There are many variants of the Transformer architecture, each with their own strengths and weaknesses. Some popular variants include:\n",
      "\n",
      "* BERT (Devlin et al., 2019) (1): A pre-trained language model that uses a multi-layer bidirectional transformer encoder.\n",
      "* RoBERTa (Liu et al., 2019) (2): A variant of BERT with additional training data and modifications to the original architecture.\n",
      "* XLNet (Yang et al., 2019) (3): A generalization of the Transformer-XL model, which includes a new method for handling long-range dependencies.\n",
      "\n",
      "These models have achieved state-of-the-art results in many NLP tasks and have been widely adopted by researchers and practitioners.\n",
      "\n",
      "References:\n",
      "\n",
      "(1) Vaswani et al. \"Attention is All You Need.\" (2017)\n",
      "\n",
      "(2) Liu et al. \"RoBERTa: A Robustly Optimized BERT Pretraining Approach.\"\n",
      "\n",
      "(3) Yang et al. \"XLNet: Generalized Autoregressive Pretraining for Language Understanding.\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Summarize the research papers about Transformer model\"}]\n",
    "})\n",
    "\n",
    "# Extract tool names and final answer\n",
    "tool_names = []\n",
    "final_answer = \"\"\n",
    "\n",
    "for msg in response.get(\"messages\", []):\n",
    "    # Capture tool calls\n",
    "    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "        for tool in msg.tool_calls:\n",
    "            tool_names.append(tool.get(\"name\", \"Unknown Tool\"))\n",
    "    # Capture final answer\n",
    "    if getattr(msg, \"content\", None) and msg.__class__.__name__ == \"AIMessage\":\n",
    "        if msg.content.strip():\n",
    "            final_answer = msg.content.strip()\n",
    "\n",
    "# Deduplicate tool names (if multiple calls)\n",
    "tool_names = list(set(tool_names))\n",
    "\n",
    "# Clean Display\n",
    "print(\"Tools Used:\", \", \".join(tool_names) if tool_names else \"None\")\n",
    "print(\"\\nAgent Response:\\n\", final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7d82184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent response: {'messages': [HumanMessage(content='Summarize the research papers about Trabsformer model ', additional_kwargs={}, response_metadata={}, id='88fa6f8a-f590-417b-9d1f-4cf8e24730b2'), AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:50:33.893908Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1654220209, 'load_duration': 60163667, 'prompt_eval_count': 910, 'prompt_eval_duration': 1043387209, 'eval_count': 20, 'eval_duration': 549344458, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--4295b2c7-a28f-4063-baad-6e8a374aa205-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Trasformer model'}, 'id': 'f891d314-edf5-46c1-957e-087f9a084159', 'type': 'tool_call'}], usage_metadata={'input_tokens': 910, 'output_tokens': 20, 'total_tokens': 930}), ToolMessage(content=\"Published: 1997-04-18\\nTitle: Renormalization trasformations of the 4D BFYM theory\\nAuthors: Alberto Accardi, Andrea Belli\\nSummary: We study the most general renormalization transformations for the first-order\\nformulation of the Yang-Mills theory. We analyze, in particular, the trivial\\nsector of the BRST cohomology of two possible formulations of the model: the\\nstandard one and the extended one. The latter is a promising starting point for\\nthe interpretation of the Yang-Mills theory as a deformation of the topological\\nBF theory. This work is a necessary preliminary step towards any perturbative\\ncalculation, and completes some recently obtained results.\\n\\nPublished: 1997-09-15\\nTitle: $σ$-models on the quantum group manifolds $SL_{q}(2,R)$, $SL_{q}(2,R)/U_{h}(1)$, $C_{q}(2|0)$ and infinitesimal trasformations\\nAuthors: V. D. Gershun\\nSummary: The differential and variational calculus on the $SL_{q}(2,R)$ group is\\nconstructed. The spontaneous breaking symmetry in the WZNW model with\\n$SL_{q}(2,R)$ quantum group symmetry and in the $\\\\sigma$-models with\\n${SL_{q}(2,R)/U_{h}(1)}$ ,$C_{q}(2|0)$ quantum group symmetry is considered.\\nThe Lagrangian formalism over the quantum group manifolds is discussed. The\\nclassical solution of $C_{q}(2|0)$ {$\\\\sigma$}-model is obtained.\\n\\nPublished: 2024-05-07\\nTitle: Darboux's Theorem, Lie series and the standardization of the Salerno and Ablowitz-Ladik models\\nAuthors: Marco Calabrese, Simone Paleari, Tiziano Penati\\nSummary: In the framework of nonlinear Hamiltonian lattices, we revisit the proof of\\nMoser-Darboux's Theorem, in order to present a general scheme for its\\nconstructive applicability to Hamiltonian models with non-standard symplectic\\nstructures. We take as a guiding example the Salerno and Ablowitz-Ladik (AL)\\nmodels: we justify the form of a well-known change of coordinates which is\\nadapted to the Gauge symmetry, by showing that it comes out in a natural way\\nwithin the general strategy outlined in the proof. Moreover, the full or\\ntruncated Lie-series technique in the extended phase-space is used to transform\\nthe Salerno model, at leading orders in the Darboux coordinates: thus the dNLS\\nHamiltonian turns out to be a normal form of the Salerno and AL models; as a\\nbyproduct we also get estimates of the dynamics of these models by means of\\ndNLS one. We also stress that, once it is cast into the perturbative approach,\\nthe method allows to deal with the cases where the explicit trasformation is\\nnot known, or even worse it is not writable in terms of elementary functions.\\n\\nPublished: 2004-11-10\\nTitle: KMS states and the chemical potential for disordered systems\\nAuthors: Francesco Fidaleo\\nSummary: We extend the theory of the chemical potential associated to a compact\\nseparable gauge group to the case of disordered quantum systems. This is done\\nin the natural framework of operator algebras. Among the other results, we show\\nthat the chemical potential does not depend on the disorder. The situation of\\nthe $n$--torus is treated in some detail. Indeed, provided that the zero--point\\nis fixed independently on the disorder, the chemical potential is intrinsically\\ndefined in terms of the direct integral decomposition of the\\nConnes--Radon--Nikodym cocycle associated to the KMS state $\\\\omega$ and its\\ntrasforms $\\\\omega\\\\circ\\\\rho$ by the localized automorphisms $\\\\rho$ of the\\nobservable algebra, carrying the abelian charges of the model under\\nconsideration. This description parallels the analogous one relative to the\\nusual (i.e. non disordered) quantum models.\\n\\nPublished: 2024-04-06\\nTitle: $f(R,\\\\square R)$-gravity and equivalency with the modified GUP Scalar field models\\nAuthors: Andronikos Paliathanasis\\nSummary: Inspired by the generalization of scalar field gravitational models with a\\nminimum length we study the equivalent theory in modified theories of gravity.\\nThe quadratic Generalized Uncertainty Principle (GUP) gives rise to a deformed\\nHeisenberg algebra in the application, resulting in the emergence of additional\", name='arxiv', id='eebc1bf3-3855-475e-a91a-b0b04229d6cf', tool_call_id='f891d314-edf5-46c1-957e-087f9a084159'), AIMessage(content=\"There are no research papers about the Trasformer model. The Trasformer model is likely a typo or misspelling of the Transformer model, which is a type of neural network architecture.\\n\\nIf you're looking for information on the Transformer model, I can provide you with some general information and summaries of relevant papers.\\n\\nThe Transformer model is a popular neural network architecture introduced in 2017 by Vaswani et al. It's primarily used for natural language processing (NLP) tasks such as machine translation, text summarization, and question answering. The model consists of an encoder and a decoder, which are composed of self-attention mechanisms that allow the model to weigh the importance of different input elements relative to each other.\\n\\nIf you'd like, I can provide more information on the Transformer model or help you find relevant papers on a specific topic related to it.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-11-02T00:50:44.325907Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8936803750, 'load_duration': 54650250, 'prompt_eval_count': 1626, 'prompt_eval_duration': 3304547333, 'eval_count': 175, 'eval_duration': 5573697750, 'model_name': 'llama3.1', 'model_provider': 'ollama'}, id='lc_run--0bc215af-c492-45e6-b40d-5c949bf279e0-0', usage_metadata={'input_tokens': 1626, 'output_tokens': 175, 'total_tokens': 1801})]}\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Summarize the research papers about Trabsformer model \"}]})\n",
    "print(\"Agent response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
